{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Speedrun: TinyStories pretraining\n",
        "\n",
        "Minimal notebook for **timed** or **full** pretraining on TinyStories on dual T4 (e.g. Kaggle).\n",
        "\n",
        "**Kaggle:** Use **GPU T4 x 2**, then run cells in order.\n",
        "\n",
        "- **Import project from GitHub:** Cell 1 detects the repo and sets paths; no extra steps.\n",
        "- **Import only this notebook:** Cell 1 will clone the full repo. Set env `KAGGLE_GITHUB_REPO` to your repo URL (e.g. `https://github.com/yourusername/project-3-systems.git`) or edit the URL in that cell.\n",
        "\n",
        "Outputs (data, checkpoints) go to `/kaggle/working/`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setup: run once. On Kaggle use GPU T4 x 2, then run this cell.\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "KAGGLE = Path(\"/kaggle\").exists()\n",
        "ROOT = Path.cwd()\n",
        "\n",
        "# If we're in notebooks/ inside the repo, go to repo root\n",
        "if not (ROOT / \"cs336_systems\").exists() and (ROOT.parent / \"cs336_systems\").exists():\n",
        "    ROOT = ROOT.parent\n",
        "# Kaggle + notebook-only import: clone repo into /kaggle/working\n",
        "elif KAGGLE and not (ROOT / \"cs336_systems\").exists():\n",
        "    repo_url = os.environ.get(\"KAGGLE_GITHUB_REPO\", \"https://github.com/YOUR_USERNAME/project-3-systems.git\")\n",
        "    repo_name = repo_url.rstrip(\"/\").split(\"/\")[-1].replace(\".git\", \"\")\n",
        "    clone_dir = Path(\"/kaggle/working\") / repo_name\n",
        "    if not clone_dir.exists():\n",
        "        print(f\"Cloning {repo_url} into {clone_dir} ...\")\n",
        "        os.chdir(\"/kaggle/working\")\n",
        "        get_ipython().system(f\"git clone --depth 1 {repo_url} {repo_name}\")\n",
        "    ROOT = clone_dir\n",
        "\n",
        "sys.path.insert(0, str(ROOT))\n",
        "os.chdir(ROOT)\n",
        "print(f\"ROOT = {ROOT}\")\n",
        "\n",
        "!pip install -q -r requirements.txt datasets transformers\n",
        "# Optional: install local cuda package if present (skip on Kaggle if not cloned)\n",
        "if (ROOT / \"cuda\").exists():\n",
        "    import subprocess\n",
        "    subprocess.run([\"pip\", \"install\", \"-q\", \"-e\", \".\"], cwd=ROOT / \"cuda\", check=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Prepare TinyStories: download, tokenize, save .pt (T4-safe: seq_len=256)\n",
        "# On Kaggle, OUT_DIR = /kaggle/working so outputs are in your notebook session.\n",
        "from pathlib import Path\n",
        "from cs336_systems.tinystories_data import build_tinystories_pt\n",
        "\n",
        "SEQ_LEN = 256\n",
        "OUT_DIR = \"/kaggle/working\" if Path(\"/kaggle\").exists() else str(ROOT)\n",
        "data_path = Path(OUT_DIR) / \"tinystories.pt\"\n",
        "\n",
        "# Set max_samples=50000 for a quicker test; None = full dataset\n",
        "if not data_path.exists():\n",
        "    build_tinystories_pt(\n",
        "        output_path=data_path,\n",
        "        seq_len=SEQ_LEN,\n",
        "        max_samples=None,\n",
        "        vocab_size=10000,\n",
        "    )\n",
        "else:\n",
        "    print(f\"Using existing {data_path}\")\n",
        "print(f\"Data path: {data_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Train: pick one.\n",
        "# Option A — Timed speedrun (e.g. 20 min on 2x T4)\n",
        "MAX_MINUTES = 20\n",
        "# Option B — Full pretraining (1 epoch over TinyStories)\n",
        "EPOCHS = 1\n",
        "USE_TIMED = True  # set False for full epoch run\n",
        "\n",
        "save_path = Path(OUT_DIR) / \"speedrun_model.pt\"\n",
        "cmd = (\n",
        "    f\"torchrun --nproc_per_node=2 train.py --ddp \"\n",
        "    f\"--config small --batch_size 4 --seq_len {SEQ_LEN} \"\n",
        "    f\"--data_path {data_path} --mixed_precision \"\n",
        "    f\"--save_path {save_path} \"\n",
        ")\n",
        "if USE_TIMED:\n",
        "    cmd += f\" --max_minutes {MAX_MINUTES}\"\n",
        "else:\n",
        "    cmd += f\" --epochs {EPOCHS}\"\n",
        "\n",
        "!{cmd}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Quick generation (single GPU, load saved checkpoint)\n",
        "import torch\n",
        "from cs336_systems.model import CONFIGS, TransformerLM\n",
        "\n",
        "if save_path.exists():\n",
        "    model = TransformerLM.from_config(\"small\", use_flash=False)\n",
        "    model.load_state_dict(torch.load(save_path, map_location=\"cpu\", weights_only=True))\n",
        "    model.eval()\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = model.to(device)\n",
        "    \n",
        "    prompt = torch.randint(0, 10000, (1, 10), device=device)\n",
        "    with torch.no_grad():\n",
        "        for _ in range(50):\n",
        "            logits = model(prompt)\n",
        "            next_id = logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
        "            prompt = torch.cat([prompt, next_id], dim=1)\n",
        "    print(\"Sample output (token ids):\", prompt[0].tolist()[:30])\n",
        "else:\n",
        "    print(\"No checkpoint found; run training cell first.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}