{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ML Systems Demo: Benchmarking & Visualization\n",
        "\n",
        "Benchmark the Transformer model (forward/backward timings, optional mixed precision) and visualize results.\n",
        "\n",
        "**Run from:** `project-3-systems/` (kernel cwd = repo root)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "ROOT = Path.cwd()\n",
        "if not (ROOT / \"cs336_systems\").exists() and (ROOT.parent / \"cs336_systems\").exists():\n",
        "    ROOT = ROOT.parent\n",
        "sys.path.insert(0, str(ROOT))\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from cs336_systems.benchmark import benchmark_model\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "if device == \"cpu\":\n",
        "    print(\"CUDA not available. Benchmark cells will be skipped (benchmark uses torch.cuda.synchronize).\")\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Benchmark: forward and forward+backward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "configs = [\"small\", \"medium\"]\n",
        "seq_lens = [128, 256, 512] if device == \"cuda\" else [128, 256]\n",
        "batch_size = 4\n",
        "n_warmup, n_steps = 3, 10\n",
        "\n",
        "rows = []\n",
        "if device != \"cuda\":\n",
        "    print(\"Skipping benchmark (requires CUDA).\")\n",
        "else:\n",
        "    for config_name in configs:\n",
        "        for seq_len in seq_lens:\n",
        "            try:\n",
        "                r_fwd = benchmark_model(config_name, batch_size, seq_len, n_warmup, n_steps, \"forward\", False, device)\n",
        "                r_bwd = benchmark_model(config_name, batch_size, seq_len, n_warmup, n_steps, \"forward_backward\", False, device)\n",
        "                rows.append({\n",
        "                    \"config\": config_name,\n",
        "                    \"seq_len\": seq_len,\n",
        "                    \"forward_ms\": r_fwd[\"mean_ms\"],\n",
        "                    \"forward_std\": r_fwd[\"std_ms\"],\n",
        "                    \"fwd_bwd_ms\": r_bwd[\"mean_ms\"],\n",
        "                    \"fwd_bwd_std\": r_bwd[\"std_ms\"],\n",
        "                    \"tokens_per_sec\": r_bwd.get(\"tokens_per_sec\", 0),\n",
        "                })\n",
        "            except Exception as e:\n",
        "                rows.append({\"config\": config_name, \"seq_len\": seq_len, \"error\": str(e)})\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "if len(df) > 0:\n",
        "    display(df)\n",
        "else:\n",
        "    df = pd.DataFrame(columns=[\"config\", \"seq_len\", \"forward_ms\", \"fwd_bwd_ms\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Visualization: timing by config and sequence length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if \"error\" not in df.columns:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "    for config_name in configs:\n",
        "        sub = df[df[\"config\"] == config_name]\n",
        "        axes[0].errorbar(sub[\"seq_len\"], sub[\"forward_ms\"], yerr=sub[\"forward_std\"], label=config_name, marker=\"o\")\n",
        "        axes[1].errorbar(sub[\"seq_len\"], sub[\"fwd_bwd_ms\"], yerr=sub[\"fwd_bwd_std\"], label=config_name, marker=\"s\")\n",
        "    axes[0].set_xlabel(\"Sequence length\")\n",
        "    axes[0].set_ylabel(\"Time (ms)\")\n",
        "    axes[0].set_title(\"Forward pass\")\n",
        "    axes[0].legend()\n",
        "    axes[1].set_xlabel(\"Sequence length\")\n",
        "    axes[1].set_ylabel(\"Time (ms)\")\n",
        "    axes[1].set_title(\"Forward + Backward\")\n",
        "    axes[1].legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Skipping plot (errors in benchmark).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Warmup vs no-warmup (CS336 A2)\n",
        "\n",
        "Compare mean forward+backward time with 0 warmup vs a few warmup steps to show effect of CUDA warmup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if device == \"cuda\":\n",
        "    w0 = benchmark_model(\"small\", 4, 256, 0, 10, \"forward_backward\", False, device)\n",
        "    w3 = benchmark_model(\"small\", 4, 256, 3, 10, \"forward_backward\", False, device)\n",
        "    print(\"No warmup (0): mean_ms =\", w0[\"mean_ms\"], \"std_ms =\", w0[\"std_ms\"])\n",
        "    print(\"Warmup (3):   mean_ms =\", w3[\"mean_ms\"], \"std_ms =\", w3[\"std_ms\"])\n",
        "else:\n",
        "    print(\"Skipping (CPU). Run on CUDA for warmup comparison.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CS336 Assignment 2 alignment\n",
        "\n",
        "This notebook runs: **benchmark script** (forward / forward+backward), **warmup vs no-warmup**, **mixed precision (FP32 vs BF16)**, and **plots**. It does **not** include Nsight reports, memory profiling scripts, or written answers.\n",
        "\n",
        "- **Full checklist:** [docs/CS336_ASSIGNMENT2_CHECKLIST.md](../docs/CS336_ASSIGNMENT2_CHECKLIST.md) — maps every A2 problem to this codebase and to writeup deliverables.\n",
        "- **Written / external:** Nsight Compute/Systems, memory profiling (record + snapshot + memory_viz), attention-only benchmark, torch.compile comparison, DDP scaling, optimizer sharding table — see checklist and complete in writeup/scripts as needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Mixed precision (BF16) comparison (CUDA only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if device == \"cuda\":\n",
        "    comp = []\n",
        "    for use_bf16 in [False, True]:\n",
        "        r = benchmark_model(\"small\", 4, 256, 3, 10, \"forward_backward\", use_bf16, device)\n",
        "        comp.append({\"mixed_precision\": \"BF16\" if use_bf16 else \"FP32\", \"mean_ms\": r[\"mean_ms\"], \"mem_mb\": r.get(\"peak_mem_mb\", 0)})\n",
        "    comp_df = pd.DataFrame(comp)\n",
        "    display(comp_df)\n",
        "    plt.bar(comp_df[\"mixed_precision\"], comp_df[\"mean_ms\"], color=[\"steelblue\", \"coral\"])\n",
        "    plt.ylabel(\"Time (ms)\")\n",
        "    plt.title(\"Forward+Backward: FP32 vs BF16\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Skipping (CPU). Run on CUDA for mixed-precision comparison.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
