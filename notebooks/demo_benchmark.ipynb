{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ML Systems Demo: Benchmarking & Visualization\n",
        "\n",
        "Benchmark the Transformer model (forward/backward timings, optional mixed precision) and visualize results.\n",
        "\n",
        "**Run from:** `project-3-systems/` (kernel cwd = repo root)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "ROOT = Path.cwd()\n",
        "if not (ROOT / \"cs336_systems\").exists() and (ROOT.parent / \"cs336_systems\").exists():\n",
        "    ROOT = ROOT.parent\n",
        "sys.path.insert(0, str(ROOT))\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from cs336_systems.benchmark import benchmark_model\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "if device == \"cpu\":\n",
        "    print(\"CUDA not available. Benchmark cells will be skipped (benchmark uses torch.cuda.synchronize).\")\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Benchmark: forward and forward+backward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "configs = [\"small\", \"medium\"]\n",
        "seq_lens = [128, 256, 512] if device == \"cuda\" else [128, 256]\n",
        "batch_size = 4\n",
        "n_warmup, n_steps = 3, 10\n",
        "\n",
        "rows = []\n",
        "if device != \"cuda\":\n",
        "    print(\"Skipping benchmark (requires CUDA).\")\n",
        "else:\n",
        "    for config_name in configs:\n",
        "        for seq_len in seq_lens:\n",
        "            try:\n",
        "                r_fwd = benchmark_model(config_name, batch_size, seq_len, n_warmup, n_steps, \"forward\", False, device)\n",
        "                r_bwd = benchmark_model(config_name, batch_size, seq_len, n_warmup, n_steps, \"forward_backward\", False, device)\n",
        "                rows.append({\n",
        "                    \"config\": config_name,\n",
        "                    \"seq_len\": seq_len,\n",
        "                    \"forward_ms\": r_fwd[\"mean_ms\"],\n",
        "                    \"forward_std\": r_fwd[\"std_ms\"],\n",
        "                    \"fwd_bwd_ms\": r_bwd[\"mean_ms\"],\n",
        "                    \"fwd_bwd_std\": r_bwd[\"std_ms\"],\n",
        "                    \"tokens_per_sec\": r_bwd.get(\"tokens_per_sec\", 0),\n",
        "                })\n",
        "            except Exception as e:\n",
        "                rows.append({\"config\": config_name, \"seq_len\": seq_len, \"error\": str(e)})\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "if len(df) > 0:\n",
        "    display(df)\n",
        "else:\n",
        "    df = pd.DataFrame(columns=[\"config\", \"seq_len\", \"forward_ms\", \"fwd_bwd_ms\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Visualization: timing by config and sequence length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if \"error\" not in df.columns:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "    for config_name in configs:\n",
        "        sub = df[df[\"config\"] == config_name]\n",
        "        axes[0].errorbar(sub[\"seq_len\"], sub[\"forward_ms\"], yerr=sub[\"forward_std\"], label=config_name, marker=\"o\")\n",
        "        axes[1].errorbar(sub[\"seq_len\"], sub[\"fwd_bwd_ms\"], yerr=sub[\"fwd_bwd_std\"], label=config_name, marker=\"s\")\n",
        "    axes[0].set_xlabel(\"Sequence length\")\n",
        "    axes[0].set_ylabel(\"Time (ms)\")\n",
        "    axes[0].set_title(\"Forward pass\")\n",
        "    axes[0].legend()\n",
        "    axes[1].set_xlabel(\"Sequence length\")\n",
        "    axes[1].set_ylabel(\"Time (ms)\")\n",
        "    axes[1].set_title(\"Forward + Backward\")\n",
        "    axes[1].legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Skipping plot (errors in benchmark).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Warmup vs no-warmup (CS336 A2)\n",
        "\n",
        "Compare mean forward+backward time with 0 warmup vs a few warmup steps to show effect of CUDA warmup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if device == \"cuda\":\n",
        "    w0 = benchmark_model(\"small\", 4, 256, 0, 10, \"forward_backward\", False, device)\n",
        "    w3 = benchmark_model(\"small\", 4, 256, 3, 10, \"forward_backward\", False, device)\n",
        "    print(\"No warmup (0): mean_ms =\", w0[\"mean_ms\"], \"std_ms =\", w0[\"std_ms\"])\n",
        "    print(\"Warmup (3):   mean_ms =\", w3[\"mean_ms\"], \"std_ms =\", w3[\"std_ms\"])\n",
        "else:\n",
        "    print(\"Skipping (CPU). Run on CUDA for warmup comparison.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CS336 Assignment 2 alignment\n",
        "\n",
        "This notebook runs: **benchmark script** (forward / forward+backward), **warmup vs no-warmup**, **mixed precision (FP32 vs BF16)**, and **plots**. It does **not** include Nsight reports, memory profiling scripts, or written answers.\n",
        "\n",
        "- **Full checklist:** [docs/CS336_ASSIGNMENT2_CHECKLIST.md](../docs/CS336_ASSIGNMENT2_CHECKLIST.md) — maps every A2 problem to this codebase and to writeup deliverables.\n",
        "- **Written / external:** Nsight Compute/Systems, memory profiling (record + snapshot + memory_viz), attention-only benchmark, torch.compile comparison, DDP scaling, optimizer sharding table — see checklist and complete in writeup/scripts as needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Mixed precision (BF16) comparison (CUDA only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if device == \"cuda\":\n",
        "    comp = []\n",
        "    for use_bf16 in [False, True]:\n",
        "        r = benchmark_model(\"small\", 4, 256, 3, 10, \"forward_backward\", use_bf16, device)\n",
        "        comp.append({\"mixed_precision\": \"BF16\" if use_bf16 else \"FP32\", \"mean_ms\": r[\"mean_ms\"], \"mem_mb\": r.get(\"peak_mem_mb\", 0)})\n",
        "    comp_df = pd.DataFrame(comp)\n",
        "    display(comp_df)\n",
        "    plt.bar(comp_df[\"mixed_precision\"], comp_df[\"mean_ms\"], color=[\"steelblue\", \"coral\"])\n",
        "    plt.ylabel(\"Time (ms)\")\n",
        "    plt.title(\"Forward+Backward: FP32 vs BF16\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Skipping (CPU). Run on CUDA for mixed-precision comparison.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Attention Benchmark (FA2 vs Naive)\n",
        "\n",
        "Compare naive attention timings across d_head and seq_len combinations (batch=8, single head). This covers the `pytorch_attention` deliverable from CS336 A2 Section 1.2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def naive_attention_unbatched(Q, K, V, is_causal=False):\n",
        "    d_k = Q.shape[-1]\n",
        "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    if is_causal:\n",
        "        N = Q.shape[-2]\n",
        "        mask = torch.triu(torch.ones(N, N, device=Q.device, dtype=torch.bool), diagonal=1)\n",
        "        scores.masked_fill_(mask, float(\"-inf\"))\n",
        "    attn = torch.softmax(scores, dim=-1)\n",
        "    return torch.matmul(attn, V)\n",
        "\n",
        "if device == \"cuda\":\n",
        "    import timeit\n",
        "    d_heads = [16, 32, 64]\n",
        "    seq_lens_attn = [256, 1024, 4096]\n",
        "    batch_attn = 8\n",
        "\n",
        "    attn_rows = []\n",
        "    for d_head in d_heads:\n",
        "        for seq_len in seq_lens_attn:\n",
        "            try:\n",
        "                Q = torch.randn(batch_attn, seq_len, d_head, device=\"cuda\")\n",
        "                K = torch.randn(batch_attn, seq_len, d_head, device=\"cuda\")\n",
        "                V = torch.randn(batch_attn, seq_len, d_head, device=\"cuda\")\n",
        "\n",
        "                for _ in range(3):\n",
        "                    naive_attention_unbatched(Q, K, V, is_causal=False)\n",
        "                    torch.cuda.synchronize()\n",
        "\n",
        "                torch.cuda.reset_peak_memory_stats()\n",
        "                times = []\n",
        "                for _ in range(10):\n",
        "                    torch.cuda.synchronize()\n",
        "                    t0 = timeit.default_timer()\n",
        "                    naive_attention_unbatched(Q, K, V, is_causal=False)\n",
        "                    torch.cuda.synchronize()\n",
        "                    times.append(timeit.default_timer() - t0)\n",
        "\n",
        "                mean_ms = sum(times) / len(times) * 1000\n",
        "                peak_mb = torch.cuda.max_memory_allocated() / (1024 * 1024)\n",
        "                attn_rows.append({\"d_head\": d_head, \"seq_len\": seq_len, \"fwd_ms\": round(mean_ms, 2), \"peak_mem_mb\": round(peak_mb, 1)})\n",
        "            except RuntimeError as e:\n",
        "                if \"out of memory\" in str(e).lower():\n",
        "                    attn_rows.append({\"d_head\": d_head, \"seq_len\": seq_len, \"fwd_ms\": \"OOM\", \"peak_mem_mb\": \"OOM\"})\n",
        "                    torch.cuda.empty_cache()\n",
        "                else:\n",
        "                    raise\n",
        "\n",
        "    attn_df = pd.DataFrame(attn_rows)\n",
        "    display(attn_df)\n",
        "else:\n",
        "    print(\"Skipping attention benchmark (requires CUDA).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Memory Profiling (Optional)\n",
        "\n",
        "Dump a CUDA memory snapshot for use with https://pytorch.org/memory_viz. Covers `memory_profiling` from CS336 A2 Section 1.1.6.\n",
        "\n",
        "To use: run the cell below, then drag the generated `.pickle` file into the memory_viz web tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if device == \"cuda\":\n",
        "    config_mem = \"small\"\n",
        "    seq_mem = 256\n",
        "    print(f\"Running memory profiling for config={config_mem}, seq_len={seq_mem}...\")\n",
        "    r = benchmark_model(config_mem, 4, seq_mem, 3, 1, \"full_step\", mixed_precision=False, device=device, memory_profile=True)\n",
        "    print(f\"Peak memory: {r.get('peak_mem_mb', 0):.1f} MB\")\n",
        "    print(f\"Memory snapshot saved to: memory_snapshot_{config_mem}_full_step_{seq_mem}.pickle\")\n",
        "    print(\"Load it at https://pytorch.org/memory_viz\")\n",
        "else:\n",
        "    print(\"Skipping memory profiling (requires CUDA).\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
